{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "462d203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4b860",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fa69b",
   "metadata": {},
   "source": [
    "### Pytorch 2.0 Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1283dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PyTorch Version\n",
    "from torch.nn.functional import scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63606bf1",
   "metadata": {},
   "source": [
    "### Self-inplemented version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f33504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    '''\n",
    "    Args:\n",
    "        p_dropout: probability of an element to be zeroed. Default: 0., i.e. no dropout\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dropout_p=0.):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        d = queries.shape[-1]\n",
    "\n",
    "        scores = (queries @ keys.transpose(-2, -1))/math.sqrt(d)\n",
    "        self.attn_weights = nn.functional.softmax(scores, dim=-1)\n",
    "\n",
    "        return self.dropout(self.attn_weights) @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ccfa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries.shape: torch.Size([2, 1, 2])\n",
      "keys.shape:    torch.Size([2, 10, 2])\n",
      "values.shape:  torch.Size([2, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "### Test Case\n",
    "queries = torch.normal(0, 1, (2,  1, 2)) ; print('queries.shape:', queries.shape)\n",
    "keys    = torch.normal(0, 1, (2, 10, 2)) ; print('keys.shape:   ', keys.shape   )\n",
    "values  = torch.normal(0, 1, (2, 10, 4)) ; print('values.shape: ', values.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e82106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DotProductAttention(\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentionMe = DotProductAttention() ; attentionMe.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ff80db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.2 µs ± 12.4 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "89.9 µs ± 3.72 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit attentionMe(queries, keys, values)\n",
    "%timeit scaled_dot_product_attention(queries, keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a72965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultMe = attentionMe(queries, keys, values)\n",
    "resultMe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff81e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of attention weights torch.Size([2, 1, 10])\n",
      "sum of attention weights of the 1st item in the batch tensor(1.0000)\n",
      "sum of attention weights of the 2nd item in the batch tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# check properties of attention weights\n",
    "print('shape of attention weights', attentionMe.attn_weights.shape)\n",
    "print('sum of attention weights of the 1st item in the batch', sum(attentionMe.attn_weights[0,0,:]))\n",
    "print('sum of attention weights of the 2nd item in the batch', sum(attentionMe.attn_weights[1,0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb5c9ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultPT = scaled_dot_product_attention(queries, keys, values)\n",
    "resultPT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78fa6459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "are_close = torch.allclose(resultMe, resultPT)\n",
    "are_close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0988eeb",
   "metadata": {},
   "source": [
    "## MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1afedf1",
   "metadata": {},
   "source": [
    "### Self-implemented with for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e280033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_forloop(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, bias=False):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim  # embedded dimension for each token in a sequence\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embed_dim % num_heads == 0, f\"Can't divide dimension {embed_dim} into {num_heads} heads\"\n",
    "\n",
    "        d_head = int(embed_dim / num_heads)\n",
    "\n",
    "        self.Wq = nn.ModuleList([nn.Linear(d_head, d_head, bias=bias) for _ in range(self.num_heads)])\n",
    "        self.Wk = nn.ModuleList([nn.Linear(d_head, d_head, bias=bias) for _ in range(self.num_heads)])\n",
    "        self.Wv = nn.ModuleList([nn.Linear(d_head, d_head, bias=bias) for _ in range(self.num_heads)])\n",
    "        self.Wo = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, embed_dim), where embed_dim = token dimension\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.num_heads):\n",
    "                Wq = self.Wq[head]\n",
    "                Wk = self.Wk[head]\n",
    "                Wv = self.Wv[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = Wq(seq), Wk(seq), Wv(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        \n",
    "        out_concat = torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
    "        \n",
    "        return self.Wo(out_concat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2dd75",
   "metadata": {},
   "source": [
    "### Self-Implemented Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eed2f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_vectorized(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_p=0., bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout_p)\n",
    "        \n",
    "        self.Wq = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.Wk = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.Wv = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.Wo = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "    \n",
    "    def forward(self, queries, keys, values, need_weights=False, average_attn_weights=False):\n",
    "        queries = self.transpose_qkv(self.Wq(queries))\n",
    "        keys    = self.transpose_qkv(self.Wq(keys))\n",
    "        values  = self.transpose_qkv(self.Wq(values))\n",
    "        \n",
    "        output = self.attention(queries, keys, values) # (batch_size*num_heads, n_seq, embed_dim/num_heads)\n",
    "        output_concat = self.transpose_output(output)  # (batch_size, n_seq, embed_dim)\n",
    "        \n",
    "        if need_weights:\n",
    "            ori_attn_weights = self.attention.attn_weights\n",
    "            ori_attn_weights = ori_attn_weights.reshape(-1, self.num_heads, ori_attn_weights.shape[1], ori_attn_weights.shape[2])\n",
    "            if average_attn_weights:\n",
    "                return self.Wo(output_concat), ori_attn_weights.mean(dim=1)\n",
    "            else: \n",
    "                return self.Wo(output_concat), ori_attn_weights\n",
    "        else:\n",
    "            return self.Wo(output_concat)\n",
    "                        \n",
    "    \n",
    "    def transpose_qkv(self, X):\n",
    "        '''Transposition for parallel computation of multiple attention heads.\n",
    "            input  X.shape = (batch_size, n_seq, embed_dim) \n",
    "            output X.shape = (batch_size*num_heads, n_seq, embed_dim/num_heads)\n",
    "        '''\n",
    "        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "    \n",
    "    def transpose_output(self, X):\n",
    "        '''Reverse the operation of transpose_qkv.'''\n",
    "        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc141d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 32, 16])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test Case\n",
    "batch_size, n_seq, embed_dim = 128, 32, 16\n",
    "num_heads = 2\n",
    "x = torch.rand(batch_size, n_seq, embed_dim) # (batch_size, n_seq, embed_dim)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "390e424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 ms ± 4.46 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "model_att_forloop = MultiHeadAttention_forloop(embed_dim, num_heads) ; model_att_forloop.eval()\n",
    "%timeit model_att_forloop(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15bb3041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.79 ms ± 22 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "model_att_vec = MultiHeadAttention_vectorized(embed_dim, num_heads) ; model_att_vec.eval()\n",
    "%timeit model_att_vec(x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7c8dd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 32, 16])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_forloop = model_att_forloop(x)\n",
    "out_forloop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bee1584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_me.shape: torch.Size([128, 32, 16])\n",
      "att_weights_me.shape: torch.Size([128, 2, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "out_me, att_weights_me = model_att_vec(x, x, x, need_weights=True, average_attn_weights=False)\n",
    "print('out_me.shape:', out_me.shape)\n",
    "print('att_weights_me.shape:', att_weights_me.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffb4bd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_me.shape: torch.Size([128, 32, 16])\n",
      "att_weights_me.shape: torch.Size([128, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "out_me, att_weights_me = model_att_vec(x, x, x, need_weights=True, average_attn_weights=True)\n",
    "print('out_me.shape:', out_me.shape)\n",
    "print('att_weights_me.shape:', att_weights_me.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b80a452",
   "metadata": {},
   "source": [
    "### Pytorch 2.0 MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c6cf95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23 ms ± 14.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "multihead_attn_PT = nn.MultiheadAttention(embed_dim, num_heads) ; multihead_attn_PT.eval()\n",
    "%timeit multihead_attn_PT(x, x, x, need_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b1c804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_PT.shape: torch.Size([128, 32, 16])\n",
      "att_weights_PT_avg.shape: torch.Size([128, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "multihead_attn_PT = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "out_PT, att_weights_PT_avg = multihead_attn_PT(x, x, x, need_weights=True, average_attn_weights=True)\n",
    "print('out_PT.shape:', out_PT.shape)\n",
    "print('att_weights_PT_avg.shape:', att_weights_PT_avg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3088aeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_PT.shape: torch.Size([128, 32, 16])\n",
      "att_weights_PT.shape: torch.Size([128, 2, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "#multihead_attn_PT = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "out_PT, att_weights_PT = multihead_attn_PT(x, x, x, need_weights=True, average_attn_weights=False)\n",
    "print('out_PT.shape:', out_PT.shape)\n",
    "print('att_weights_PT.shape:', att_weights_PT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc75a3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(att_weights_PT.mean(dim=1), att_weights_PT_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ac883",
   "metadata": {},
   "source": [
    "## ------ END ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd45d195",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b2d947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
